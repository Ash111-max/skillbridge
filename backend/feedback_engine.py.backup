# # # feedback_engine.py

# # import os
# # import re
# # from typing import Dict, Any, Optional
# # import language_tool_python
# # from textblob import TextBlob

# # # sentence-transformers for semantic similarity
# # try:
# #     from sentence_transformers import SentenceTransformer, util as sbert_util
# #     SBERT = SentenceTransformer("all-MiniLM-L6-v2")
# #     print("✅ Loaded SentenceTransformer (all-MiniLM-L6-v2).")
# # except Exception as e:
# #     SBERT = None
# #     print("⚠️ SBERT not available:", e)

# # # Transformers sentiment fallback
# # try:
# #     from transformers import pipeline
# #     sentiment_pipeline = pipeline("sentiment-analysis")
# #     print("✅ Loaded sentiment pipeline.")
# # except Exception as e:
# #     sentiment_pipeline = None
# #     print("⚠️ Sentiment pipeline unavailable, falling back to TextBlob.", e)

# # FILLER_WORDS = set(["um", "uh", "like", "you know", "actually", "basically", "literally", "sort of", "kinda", "stuff", "hmm", "erm"])
# # SLANG_WORDS = set(["yeah", "gonna", "wanna", "gotta", "bro", "dude", "lol", "btw", "cuz", "ain't"])


# # def _count_words(text: str):
# #     return len([w for w in re.findall(r"\b\w+\b", text)])


# # def _detect_fillers(text: str):
# #     found = re.findall(r"\b(" + "|".join(re.escape(w) for w in FILLER_WORDS) + r")\b", text, flags=re.I)
# #     return [w.lower() for w in found]


# # def _detect_slang(text: str):
# #     found = re.findall(r"\b(" + "|".join(re.escape(w) for w in SLANG_WORDS) + r")\b", text, flags=re.I)
# #     return [w.lower() for w in found]


# # def semantic_similarity_score(candidate: str, reference: Optional[str]):
# #     if not reference:
# #         return None
# #     if not SBERT:
# #         cset = set(re.findall(r"\w+", candidate.lower()))
# #         rset = set(re.findall(r"\w+", reference.lower()))
# #         inter = len(cset & rset)
# #         union = max(1, len(rset))
# #         frac = inter / union
# #         return round(min(10, frac * 10), 1)
# #     try:
# #         emb1 = SBERT.encode(candidate, convert_to_tensor=True)
# #         emb2 = SBERT.encode(reference, convert_to_tensor=True)
# #         cos = sbert_util.cos_sim(emb1, emb2).item()
# #         score = max(0.0, (cos + 1) / 2 * 10)
# #         return round(min(10, score), 2)
# #     except Exception as e:
# #         print("Semantic scoring failed:", e)
# #         return None


# # def sentiment_analysis(text: str):
# #     if sentiment_pipeline:
# #         try:
# #             res = sentiment_pipeline(text)[0]
# #             label = res.get("label", "")
# #             base_score = res.get("score", 0.5)
# #             score = 8 if label.lower().startswith("pos") else 5 if label.lower().startswith("neu") else 3
# #             return {"label": label.capitalize(), "confidence": round(base_score, 3), "mapped_score": score}
# #         except Exception as e:
# #             print("Sentiment pipeline error:", e)
# #     blob = TextBlob(text)
# #     polarity = blob.sentiment.polarity
# #     label = "Positive" if polarity > 0.2 else "Negative" if polarity < -0.2 else "Neutral"
# #     mapped = 8 if polarity > 0.2 else 3 if polarity < -0.2 else 5
# #     return {"label": label, "polarity": round(polarity, 3), "mapped_score": mapped}


# # # def compute_fluency_score(text: str):
# # #     sentences = [s.strip() for s in re.split(r"[.!?]+", text) if s.strip()]
# # #     avg_len = sum(len(s.split()) for s in sentences) / max(1, len(sentences))
# # #     punctuation = len(re.findall(r"[,.!?;:]", text))
# # #     score = (min(1, avg_len / 18) * 6) + (min(1, punctuation / 5) * 4)
# # #     return round(min(10, max(0, score)), 1)

# # def compute_fluency_score(text: str, fillers: list):
# #     """Assess fluency based on sentence flow and filler usage."""
# #     sentences = [s.strip() for s in re.split(r"[.!?]+", text) if s.strip()]
# #     word_count = sum(len(s.split()) for s in sentences)
# #     avg_sentence_len = word_count / max(1, len(sentences))
    
# #     # Fewer fillers + smoother punctuation = better fluency
# #     filler_penalty = len(fillers) * 0.8
# #     punctuation = len(re.findall(r"[,.!?;:]", text))
# #     smoothness = min(10, (avg_sentence_len / 12) * 6 + (punctuation / 4) * 4)
# #     score = smoothness - filler_penalty
    
# #     return round(max(0, min(10, score)), 1)

# # # def compute_grammar_score(text: str):
# # #     try:
# # #         blob = TextBlob(text)
# # #         corrected = str(blob.correct())
# # #         diff_chars = sum(1 for a, b in zip(text.lower(), corrected.lower()) if a != b)
# # #         norm = diff_chars / max(1, len(text))
# # #         score = max(0, 10 - norm * 50)
# # #         return round(min(10, score), 1), corrected
# # #     except Exception as e:
# # #         print("Grammar check fallback:", e)
# # #         return 7.5, text

# # def compute_grammar_score(text: str):
# #     """Evaluate grammar using LanguageTool, fallback to TextBlob if not available."""
# #     try:
# #         tool = language_tool_python.LanguageTool('en-US')
# #         matches = tool.check(text)
# #         issue_count = len(matches)
# #         words = len(re.findall(r"\b\w+\b", text))
# #         ratio = issue_count / max(1, words)
# #         score = max(0, 10 - ratio * 100)  # each error per 100 words = -1 point
# #         tool.close()
# #         return round(min(10, score), 1), text
# #     except Exception as e:
# #         print("⚠️ LanguageTool unavailable, using fallback:", e)
# #         from textblob import TextBlob
# #         blob = TextBlob(text)
# #         corrected = str(blob.correct())
# #         diff_chars = sum(1 for a, b in zip(text.lower(), corrected.lower()) if a != b)
# #         norm = diff_chars / max(1, len(text))
# #         score = max(0, 10 - norm * 20)  # smaller penalty
# #         return round(min(10, score), 1), text


# # def aggregate_scores(grammar, fluency, confidence_score, sentiment_mapped, filler_count, semantic_score=None):
# #     weights = {"grammar": 0.25, "fluency": 0.2, "confidence": 0.2, "sentiment": 0.15, "filler": 0.1, "semantic": 0.1}
# #     filler_pen = max(0, 10 - filler_count * 1.5)
# #     base = (grammar * weights["grammar"] +
# #             fluency * weights["fluency"] +
# #             confidence_score * weights["confidence"] +
# #             sentiment_mapped * weights["sentiment"] +
# #             filler_pen * weights["filler"])
# #     if semantic_score is not None:
# #         base = base * (1 - weights["semantic"]) + semantic_score * weights["semantic"]
# #     return round(min(10, max(0, base)), 2)


# # def generate_tips(feedback: Dict[str, Any]):
# #     tips = []
# #     grammar_score = feedback["grammar"]["score"] if isinstance(feedback["grammar"], dict) else feedback["grammar"]
    
# #     if grammar_score < 7:
# #         tips.append("Work on grammar — read answers out loud and use short sentences.")
# #     if feedback["fluency_score"] < 6:
# #         tips.append("Practice speaking in full sentences and avoid long pauses.")
# #     if feedback["filler_word_count"] > 2:
# #         tips.append("Reduce filler words like 'um' and 'like' — try pausing instead.")
# #     if feedback["confidence_score"] < 6:
# #         tips.append("Add a short example or personal story to show confidence.")
# #     if feedback.get("semantic_score") is not None and feedback["semantic_score"] < 6:
# #         tips.append("Include key domain points. Compare your answer to model points.")
# #     if not tips:
# #         tips.append("Nice work — minor refinements can take you further.")
# #     return tips


# # def generate_overall_feedback(feedback: Dict[str, Any]) -> str:
# #     """Generate human-readable overall feedback summary"""
# #     grammar_score = feedback["grammar"]["score"] if isinstance(feedback["grammar"], dict) else feedback["grammar"]
# #     fluency = feedback["fluency_score"]
# #     confidence = feedback["confidence_score"]
    
# #     feedback_parts = []
    
# #     # Grammar feedback
# #     if grammar_score >= 8:
# #         feedback_parts.append("Your grammar was excellent.")
# #     elif grammar_score >= 6:
# #         feedback_parts.append("Your grammar was good with minor issues.")
# #     else:
# #         feedback_parts.append("Consider reviewing grammar basics.")
    
# #     # Fluency feedback
# #     if fluency >= 8:
# #         feedback_parts.append("You spoke very fluently.")
# #     elif fluency >= 6:
# #         feedback_parts.append("Your fluency was decent.")
# #     else:
# #         feedback_parts.append("Try to speak more smoothly without long pauses.")
    
# #     # Confidence feedback
# #     if confidence >= 8:
# #         feedback_parts.append("You demonstrated strong confidence.")
# #     elif confidence >= 6:
# #         feedback_parts.append("Your confidence level was moderate.")
# #     else:
# #         feedback_parts.append("Work on building more confidence in your delivery.")
    
# #     # Filler words
# #     if feedback["filler_word_count"] > 3:
# #         feedback_parts.append(f"Try to reduce filler words (you used {feedback['filler_word_count']}).")
    
# #     return " ".join(feedback_parts)


# # def analyze_response(text: str, audio_meta: Optional[dict] = None, reference_answer: Optional[str] = None) -> Dict[str, Any]:
# #     if not text or not text.strip():
# #         return {"error": "No valid response detected.", "suggestion": "Try speaking louder or type your answer."}

# #     sentiment = sentiment_analysis(text)
# #     fillers = _detect_fillers(text)
# #     slang = _detect_slang(text)
# #     grammar_score, corrected_text = compute_grammar_score(text)
# #     # fluency_score = compute_fluency_score(text)
# #     fluency_score = compute_fluency_score(text, fillers)
# #     words = _count_words(text)

# #     if audio_meta and audio_meta.get("duration"):
# #         wpm = words / (audio_meta["duration"] / 60.0)
# #     else:
# #         wpm = words / 0.5 if words else 0

# #     confidence_score = 10 if wpm > 120 else 8 if wpm > 90 else 6 if wpm > 60 else 4 if wpm > 30 else 2

# #     tone = {}
# #     if audio_meta:
# #         avg_pitch = audio_meta.get("avg_pitch")
# #         pitch_var = audio_meta.get("pitch_var")
# #         rms = audio_meta.get("rms")
# #         nervous = (pitch_var is not None and pitch_var > 50) or (wpm < 40 and rms < 0.01)
# #         calm = (pitch_var is not None and pitch_var < 30 and rms > 0.01 and wpm > 60)
# #         tone["nervous_score"] = 8 if nervous else 3 if calm else 5
# #         tone["avg_pitch"] = avg_pitch
# #         tone["pitch_var"] = pitch_var
# #         tone["rms"] = rms

# #     semantic_score = semantic_similarity_score(text, reference_answer)
# #     overall = aggregate_scores(grammar_score, fluency_score, confidence_score, sentiment.get("mapped_score", 5), len(fillers), semantic_score)

# #     feedback = {
# #         "text_analyzed": text,
# #         "grammar": {
# #             "score": grammar_score,
# #             "suggested_text": corrected_text,
# #             "mapped_score": grammar_score / 10,  # Normalize to 0-1 for consistency
# #             "confidence": grammar_score / 10  # Add confidence field for UI compatibility
# #         },
# #         "fluency_score": fluency_score,
# #         "clarity_score": fluency_score,  # Alias for UI
# #         "filler_word_count": len(fillers),
# #         "filler_words": list(set(fillers)),
# #         "professionalism_warning": ", ".join(set(slang)) if slang else None,
# #         "confidence_score": confidence_score,
# #         "pacing_score": confidence_score,  # Alias for UI (since pacing relates to WPM)
# #         "sentiment": sentiment,
# #         "tone": sentiment.get("label", "Neutral"),  # Add tone field for UI
# #         "semantic_score": semantic_score,
# #         "voice_tone": tone,
# #         "overall_performance": overall,
# #         "word_count": words,
# #         "duration": audio_meta.get("duration") if audio_meta else None,
# #         "avg_pitch": audio_meta.get("avg_pitch") if audio_meta else None,
# #     }

# #     # Generate overall feedback text
# #     feedback["overall_feedback"] = generate_overall_feedback(feedback)
# #     feedback["improvement_tips"] = generate_tips(feedback)
    
# #     return feedback

# # feedback_engine.py

# import os
# import re
# from typing import Dict, Any, Optional
# from textblob import TextBlob

# # sentence-transformers for semantic similarity
# try:
#     from sentence_transformers import SentenceTransformer, util as sbert_util
#     SBERT = SentenceTransformer("all-MiniLM-L6-v2")
#     print("✅ Loaded SentenceTransformer (all-MiniLM-L6-v2).")
# except Exception as e:
#     SBERT = None
#     print("⚠️ SBERT not available:", e)

# # Transformers sentiment fallback
# try:
#     from transformers import pipeline
#     sentiment_pipeline = pipeline("sentiment-analysis")
#     print("✅ Loaded sentiment pipeline.")
# except Exception as e:
#     sentiment_pipeline = None
#     print("⚠️ Sentiment pipeline unavailable, falling back to TextBlob.", e)

# FILLER_WORDS = set(["um", "uh", "like", "you know", "actually", "basically", "literally", "sort of", "kinda", "stuff", "hmm", "erm"])
# SLANG_WORDS = set(["yeah", "gonna", "wanna", "gotta", "bro", "dude", "lol", "btw", "cuz", "ain't"])


# def _count_words(text: str):
#     return len([w for w in re.findall(r"\b\w+\b", text)])


# def _detect_fillers(text: str):
#     found = re.findall(r"\b(" + "|".join(re.escape(w) for w in FILLER_WORDS) + r")\b", text, flags=re.I)
#     return [w.lower() for w in found]


# def _detect_slang(text: str):
#     found = re.findall(r"\b(" + "|".join(re.escape(w) for w in SLANG_WORDS) + r")\b", text, flags=re.I)
#     return [w.lower() for w in found]


# def semantic_similarity_score(candidate: str, reference: Optional[str]):
#     if not reference:
#         return None
#     if not SBERT:
#         cset = set(re.findall(r"\w+", candidate.lower()))
#         rset = set(re.findall(r"\w+", reference.lower()))
#         inter = len(cset & rset)
#         union = max(1, len(rset))
#         frac = inter / union
#         return round(min(10, frac * 10), 1)
#     try:
#         emb1 = SBERT.encode(candidate, convert_to_tensor=True)
#         emb2 = SBERT.encode(reference, convert_to_tensor=True)
#         cos = sbert_util.cos_sim(emb1, emb2).item()
#         score = max(0.0, (cos + 1) / 2 * 10)
#         return round(min(10, score), 2)
#     except Exception as e:
#         print("Semantic scoring failed:", e)
#         return None


# def sentiment_analysis(text: str):
#     if sentiment_pipeline:
#         try:
#             res = sentiment_pipeline(text)[0]
#             label = res.get("label", "")
#             base_score = res.get("score", 0.5)
#             score = 8 if label.lower().startswith("pos") else 5 if label.lower().startswith("neu") else 3
#             return {"label": label.capitalize(), "confidence": round(base_score, 3), "mapped_score": score}
#         except Exception as e:
#             print("Sentiment pipeline error:", e)
#     blob = TextBlob(text)
#     polarity = blob.sentiment.polarity
#     label = "Positive" if polarity > 0.2 else "Negative" if polarity < -0.2 else "Neutral"
#     mapped = 8 if polarity > 0.2 else 3 if polarity < -0.2 else 5
#     return {"label": label, "polarity": round(polarity, 3), "mapped_score": mapped}


# def compute_fluency_score(text: str):
#     sentences = [s.strip() for s in re.split(r"[.!?]+", text) if s.strip()]
#     avg_len = sum(len(s.split()) for s in sentences) / max(1, len(sentences))
#     punctuation = len(re.findall(r"[,.!?;:]", text))
#     score = (min(1, avg_len / 18) * 6) + (min(1, punctuation / 5) * 4)
#     return round(min(10, max(0, score)), 1)


# def compute_grammar_score(text: str):
#     try:
#         blob = TextBlob(text)
#         corrected = str(blob.correct())
#         diff_chars = sum(1 for a, b in zip(text.lower(), corrected.lower()) if a != b)
#         norm = diff_chars / max(1, len(text))
#         score = max(0, 10 - norm * 50)
#         return round(min(10, score), 1), corrected
#     except Exception as e:
#         print("Grammar check fallback:", e)
#         return 7.5, text


# def aggregate_scores(grammar, fluency, confidence_score, sentiment_mapped, filler_count, semantic_score=None):
#     weights = {"grammar": 0.25, "fluency": 0.2, "confidence": 0.2, "sentiment": 0.15, "filler": 0.1, "semantic": 0.1}
#     filler_pen = max(0, 10 - filler_count * 1.5)
#     base = (grammar * weights["grammar"] +
#             fluency * weights["fluency"] +
#             confidence_score * weights["confidence"] +
#             sentiment_mapped * weights["sentiment"] +
#             filler_pen * weights["filler"])
#     if semantic_score is not None:
#         base = base * (1 - weights["semantic"]) + semantic_score * weights["semantic"]
#     return round(min(10, max(0, base)), 2)


# def generate_tips(feedback: Dict[str, Any]):
#     tips = []
#     grammar_score = feedback["grammar"]["score"] if isinstance(feedback["grammar"], dict) else feedback["grammar"]
    
#     if grammar_score < 7:
#         tips.append("Work on grammar — read answers out loud and use short sentences.")
#     if feedback["fluency_score"] < 6:
#         tips.append("Practice speaking in full sentences and avoid long pauses.")
#     if feedback["filler_word_count"] > 2:
#         tips.append("Reduce filler words like 'um' and 'like' — try pausing instead.")
#     if feedback["confidence_score"] < 6:
#         tips.append("Add a short example or personal story to show confidence.")
#     if feedback.get("semantic_score") is not None and feedback["semantic_score"] < 6:
#         tips.append("Include key domain points. Compare your answer to model points.")
#     if not tips:
#         tips.append("Nice work — minor refinements can take you further.")
#     return tips


# def generate_overall_feedback(feedback: Dict[str, Any]) -> str:
#     """Generate human-readable overall feedback summary"""
#     grammar_score = feedback["grammar"]["score"] if isinstance(feedback["grammar"], dict) else feedback["grammar"]
#     fluency = feedback["fluency_score"]
#     confidence = feedback["confidence_score"]
    
#     feedback_parts = []
    
#     # Grammar feedback
#     if grammar_score >= 8:
#         feedback_parts.append("Your grammar was excellent.")
#     elif grammar_score >= 6:
#         feedback_parts.append("Your grammar was good with minor issues.")
#     else:
#         feedback_parts.append("Consider reviewing grammar basics.")
    
#     # Fluency feedback
#     if fluency >= 8:
#         feedback_parts.append("You spoke very fluently.")
#     elif fluency >= 6:
#         feedback_parts.append("Your fluency was decent.")
#     else:
#         feedback_parts.append("Try to speak more smoothly without long pauses.")
    
#     # Confidence feedback
#     if confidence >= 8:
#         feedback_parts.append("You demonstrated strong confidence.")
#     elif confidence >= 6:
#         feedback_parts.append("Your confidence level was moderate.")
#     else:
#         feedback_parts.append("Work on building more confidence in your delivery.")
    
#     # Filler words
#     if feedback["filler_word_count"] > 3:
#         feedback_parts.append(f"Try to reduce filler words (you used {feedback['filler_word_count']}).")
    
#     return " ".join(feedback_parts)


# def analyze_response(text: str, audio_meta: Optional[dict] = None, reference_answer: Optional[str] = None) -> Dict[str, Any]:
#     if not text or not text.strip():
#         return {"error": "No valid response detected.", "suggestion": "Try speaking louder or type your answer."}

#     sentiment = sentiment_analysis(text)
#     fillers = _detect_fillers(text)
#     slang = _detect_slang(text)
#     grammar_score, corrected_text = compute_grammar_score(text)
#     fluency_score = compute_fluency_score(text)
#     words = _count_words(text)

#     if audio_meta and audio_meta.get("duration"):
#         wpm = words / (audio_meta["duration"] / 60.0)
#     else:
#         wpm = words / 0.5 if words else 0

#     confidence_score = 10 if wpm > 120 else 8 if wpm > 90 else 6 if wpm > 60 else 4 if wpm > 30 else 2

#     tone = {}
#     if audio_meta:
#         avg_pitch = audio_meta.get("avg_pitch")
#         pitch_var = audio_meta.get("pitch_var")
#         rms = audio_meta.get("rms")
#         nervous = (pitch_var is not None and pitch_var > 50) or (wpm < 40 and rms < 0.01)
#         calm = (pitch_var is not None and pitch_var < 30 and rms > 0.01 and wpm > 60)
#         tone["nervous_score"] = 8 if nervous else 3 if calm else 5
#         tone["avg_pitch"] = avg_pitch
#         tone["pitch_var"] = pitch_var
#         tone["rms"] = rms

#     semantic_score = semantic_similarity_score(text, reference_answer)
#     overall = aggregate_scores(grammar_score, fluency_score, confidence_score, sentiment.get("mapped_score", 5), len(fillers), semantic_score)

#     feedback = {
#         "text_analyzed": text,
#         "grammar": {
#             "score": grammar_score,
#             "suggested_text": corrected_text,
#             "mapped_score": grammar_score / 10,  # Normalize to 0-1 for consistency
#             "confidence": grammar_score / 10  # Add confidence field for UI compatibility
#         },
#         "fluency_score": fluency_score,
#         "clarity_score": fluency_score,  # Alias for UI
#         "filler_word_count": len(fillers),
#         "filler_words": list(set(fillers)),
#         "professionalism_warning": ", ".join(set(slang)) if slang else None,
#         "confidence_score": confidence_score,
#         "pacing_score": confidence_score,  # Alias for UI (since pacing relates to WPM)
#         "sentiment": sentiment,
#         "tone": sentiment.get("label", "Neutral"),  # Add tone field for UI
#         "semantic_score": semantic_score,
#         "voice_tone": tone,
#         "overall_performance": overall,
#         "word_count": words,
#         "duration": audio_meta.get("duration") if audio_meta else None,
#         "avg_pitch": audio_meta.get("avg_pitch") if audio_meta else None,
#     }

#     # Generate overall feedback text
#     feedback["overall_feedback"] = generate_overall_feedback(feedback)
#     feedback["improvement_tips"] = generate_tips(feedback)
    
#     return feedback

import os
import re
from typing import Dict, Any, Optional, List
from textblob import TextBlob
import language_tool_python

# =================== LOAD MODELS ===================

# Sentence-BERT for semantic similarity
try:
    from sentence_transformers import SentenceTransformer, util as sbert_util
    SBERT = SentenceTransformer("all-MiniLM-L6-v2")
    print("✅ Loaded SentenceTransformer (all-MiniLM-L6-v2)")
except Exception as e:
    SBERT = None
    print("⚠️ SBERT not available:", e)

# Transformers sentiment analysis
try:
    from transformers import pipeline
    sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
    print("✅ Loaded sentiment pipeline")
except Exception as e:
    sentiment_pipeline = None
    print("⚠️ Sentiment pipeline unavailable, falling back to TextBlob")

# LanguageTool for better grammar checking
try:
    grammar_tool = language_tool_python.LanguageTool('en-US')
    print("✅ Loaded LanguageTool for grammar checking")
except Exception as e:
    grammar_tool = None
    print("⚠️ LanguageTool unavailable, using TextBlob fallback")

# =================== CONSTANTS ===================

FILLER_WORDS = {
    "um", "uh", "like", "you know", "actually", "basically", "literally",
    "sort of", "kind of", "kinda", "stuff", "hmm", "erm", "ah", "er"
}

SLANG_WORDS = {
    "yeah", "gonna", "wanna", "gotta", "bro", "dude", "lol", "btw",
    "cuz", "ain't", "dunno", "lemme", "gimme"
}

# Words per minute ranges
WPM_OPTIMAL_MIN = 120
WPM_OPTIMAL_MAX = 160
WPM_ACCEPTABLE_MIN = 100
WPM_ACCEPTABLE_MAX = 180


# =================== HELPER FUNCTIONS ===================

def _count_words(text: str) -> int:
    """Count words in text"""
    return len(re.findall(r"\b\w+\b", text))


def _count_sentences(text: str) -> int:
    """Count sentences in text"""
    sentences = re.split(r'[.!?]+', text)
    return len([s for s in sentences if s.strip()])


def _detect_fillers(text: str) -> List[str]:
    """Detect filler words"""
    words = re.findall(r"\b\w+\b", text.lower())
    return [w for w in words if w in FILLER_WORDS]


def _detect_slang(text: str) -> List[str]:
    """Detect slang words"""
    words = re.findall(r"\b\w+\b", text.lower())
    return [w for w in words if w in SLANG_WORDS]


def _calculate_wpm(word_count: int, duration: float) -> float:
    """Calculate words per minute"""
    if duration <= 0:
        return 0
    return (word_count / duration) * 60


# =================== SCORING FUNCTIONS ===================

def compute_clarity_score(text: str, audio_meta: Dict[str, Any]) -> Dict[str, Any]:
    """
    Improved clarity scoring based on:
    - Sentence structure
    - Vocabulary richness
    - Appropriate punctuation
    - Audio quality metrics
    """
    if not text or len(text.strip()) < 3:
        return {"score": 0.0, "feedback": "Response too short to analyze"}
    
    word_count = _count_words(text)
    sentence_count = _count_sentences(text)
    
    # Base score from structure
    if sentence_count == 0:
        sentence_count = 1
    
    avg_words_per_sentence = word_count / sentence_count
    
    # Optimal: 10-20 words per sentence
    if 10 <= avg_words_per_sentence <= 20:
        structure_score = 10
    elif 7 <= avg_words_per_sentence <= 25:
        structure_score = 8
    elif 5 <= avg_words_per_sentence <= 30:
        structure_score = 6
    else:
        structure_score = 4
    
    # Vocabulary richness (unique words ratio)
    words = re.findall(r"\b\w+\b", text.lower())
    unique_ratio = len(set(words)) / max(1, len(words))
    vocab_score = min(10, unique_ratio * 12)
    
    # Punctuation usage
    punctuation_count = len(re.findall(r"[,.!?;:]", text))
    punct_score = min(10, (punctuation_count / max(1, sentence_count)) * 5)
    
    # Audio clarity (if available)
    audio_score = 8  # Default
    if audio_meta:
        pitch_var = audio_meta.get("pitch_var", 0)
        rms = audio_meta.get("rms", 0)
        
        # Pitch variation indicates expressiveness
        if pitch_var and pitch_var > 100:
            audio_score = min(10, 7 + (pitch_var / 500))
    
    # Weighted average
    clarity_score = (
        structure_score * 0.35 +
        vocab_score * 0.25 +
        punct_score * 0.20 +
        audio_score * 0.20
    )
    
    # Bonus for longer, coherent responses
    if word_count > 30:
        clarity_score = min(10, clarity_score + 0.5)
    
    feedback = []
    if structure_score < 6:
        feedback.append("Try using more complete sentences")
    if vocab_score < 6:
        feedback.append("Use more varied vocabulary")
    if punct_score < 5:
        feedback.append("Add pauses and punctuation for clarity")
    
    return {
        "score": round(clarity_score, 1),
        "feedback": "; ".join(feedback) if feedback else "Clear and well-structured response"
    }


def compute_fluency_score(text: str, filler_count: int, audio_meta: Dict[str, Any]) -> Dict[str, Any]:
    """
    Improved fluency scoring based on:
    - Natural flow (sentence length variation)
    - Minimal filler words
    - Appropriate pacing
    - Smooth delivery
    """
    if not text or len(text.strip()) < 3:
        return {"score": 0.0, "feedback": "Response too short to analyze"}
    
    word_count = _count_words(text)
    sentence_count = _count_sentences(text)
    
    # Sentence length variation (good fluency has variety)
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    if len(sentences) > 1:
        sentence_lengths = [len(s.split()) for s in sentences]
        avg_length = sum(sentence_lengths) / len(sentence_lengths)
        variance = sum((x - avg_length) ** 2 for x in sentence_lengths) / len(sentence_lengths)
        variation_score = min(10, 5 + (variance / 10))
    else:
        variation_score = 7  # Neutral for single sentence
    
    # Filler word penalty
    filler_ratio = filler_count / max(1, word_count)
    filler_score = max(0, 10 - (filler_ratio * 50))
    
    # Pacing from audio
    pacing_score = 8  # Default
    if audio_meta and audio_meta.get("duration"):
        wpm = _calculate_wpm(word_count, audio_meta["duration"])
        
        if WPM_OPTIMAL_MIN <= wpm <= WPM_OPTIMAL_MAX:
            pacing_score = 10
        elif WPM_ACCEPTABLE_MIN <= wpm <= WPM_ACCEPTABLE_MAX:
            pacing_score = 8
        elif wpm < WPM_ACCEPTABLE_MIN:
            pacing_score = 6  # Too slow
        else:
            pacing_score = 5  # Too fast
    
    # Natural flow (coherence)
    coherence_score = 8
    if word_count < 10:
        coherence_score = 5  # Too short
    elif word_count > 50:
        coherence_score = 9  # Good length
    
    # Weighted average
    fluency_score = (
        variation_score * 0.25 +
        filler_score * 0.30 +
        pacing_score * 0.25 +
        coherence_score * 0.20
    )
    
    feedback = []
    if filler_score < 6:
        feedback.append(f"Reduce filler words (found {filler_count})")
    if pacing_score < 7 and audio_meta:
        wpm = _calculate_wpm(word_count, audio_meta.get("duration", 1))
        if wpm < WPM_ACCEPTABLE_MIN:
            feedback.append("Try speaking a bit faster")
        elif wpm > WPM_ACCEPTABLE_MAX:
            feedback.append("Slow down for better clarity")
    
    return {
        "score": round(fluency_score, 1),
        "feedback": "; ".join(feedback) if feedback else "Smooth and natural delivery"
    }


def compute_grammar_score(text: str) -> Dict[str, Any]:
    """
    Improved grammar scoring using LanguageTool
    More lenient for short responses and conversational style
    """
    if not text or len(text.strip()) < 3:
        return {"score": 0.0, "errors": 0, "feedback": "Response too short to analyze"}
    
    word_count = _count_words(text)
    
    # Use LanguageTool if available (more accurate)
    if grammar_tool:
        try:
            matches = grammar_tool.check(text)
            
            # Filter out minor issues for short responses
            significant_errors = [
                m for m in matches 
                if m.ruleId not in ['WHITESPACE_RULE', 'UPPERCASE_SENTENCE_START']
            ]
            
            error_count = len(significant_errors)
            
            # More lenient scoring
            if word_count < 15:
                # Short responses: max 2 errors acceptable
                if error_count == 0:
                    score = 10
                elif error_count == 1:
                    score = 8
                elif error_count == 2:
                    score = 6
                else:
                    score = max(3, 10 - error_count * 2)
            else:
                # Longer responses: error ratio matters
                error_ratio = error_count / word_count
                score = max(0, 10 - (error_ratio * 40))
            
            return {
                "score": round(score, 1),
                "errors": error_count,
                "details": [{"message": m.message, "suggestion": m.replacements[:2]} for m in significant_errors[:3]],
                "feedback": "Excellent grammar" if error_count == 0 else f"Minor grammar issues ({error_count} found)"
            }
        except Exception as e:
            print(f"LanguageTool error: {e}")
    
    # Fallback to TextBlob (less accurate but faster)
    try:
        blob = TextBlob(text)
        corrected = str(blob.correct())
        
        # Count character-level differences
        diff_count = sum(1 for a, b in zip(text.lower(), corrected.lower()) if a != b)
        diff_ratio = diff_count / max(1, len(text))
        
        # More lenient scoring
        if diff_ratio < 0.05:  # Less than 5% difference
            score = 9
        elif diff_ratio < 0.10:
            score = 7
        elif diff_ratio < 0.15:
            score = 5
        else:
            score = max(3, 10 - diff_ratio * 30)
        
        return {
            "score": round(score, 1),
            "errors": diff_count,
            "feedback": "Good grammar" if diff_ratio < 0.10 else "Some grammar improvements needed"
        }
    except Exception as e:
        print(f"Grammar check failed: {e}")
        return {"score": 7.0, "errors": 0, "feedback": "Grammar check unavailable"}


def compute_confidence_score(audio_meta: Dict[str, Any], word_count: int) -> Dict[str, Any]:
    """
    Confidence based on:
    - Speaking pace (WPM)
    - Pitch variation (expressiveness)
    - Volume consistency (RMS)
    - Response completeness
    """
    if not audio_meta:
        return {"score": 7.0, "feedback": "Audio metrics unavailable"}
    
    duration = audio_meta.get("duration", 0)
    pitch_var = audio_meta.get("pitch_var", 0)
    rms = audio_meta.get("rms", 0)
    
    if duration <= 0:
        return {"score": 5.0, "feedback": "Response too short"}
    
    # Speaking pace confidence
    wpm = _calculate_wpm(word_count, duration)
    if WPM_OPTIMAL_MIN <= wpm <= WPM_OPTIMAL_MAX:
        pace_score = 10
    elif WPM_ACCEPTABLE_MIN <= wpm <= WPM_ACCEPTABLE_MAX:
        pace_score = 8
    else:
        pace_score = 6
    
    # Pitch variation (confident speakers vary pitch)
    if pitch_var:
        if pitch_var > 200:
            pitch_score = 10
        elif pitch_var > 100:
            pitch_score = 8
        elif pitch_var > 50:
            pitch_score = 6
        else:
            pitch_score = 4
    else:
        pitch_score = 7
    
    # Volume consistency (confident speakers have steady volume)
    if rms > 0.01:
        volume_score = 9
    elif rms > 0.005:
        volume_score = 7
    else:
        volume_score = 5
    
    # Response completeness
    if word_count > 30:
        completeness_score = 10
    elif word_count > 15:
        completeness_score = 8
    elif word_count > 5:
        completeness_score = 6
    else:
        completeness_score = 4
    
    confidence_score = (
        pace_score * 0.30 +
        pitch_score * 0.30 +
        volume_score * 0.20 +
        completeness_score * 0.20
    )
    
    feedback = []
    if pace_score < 7:
        feedback.append("Maintain steady speaking pace")
    if pitch_score < 6:
        feedback.append("Add more expression to your voice")
    if completeness_score < 7:
        feedback.append("Provide more detailed responses")
    
    return {
        "score": round(confidence_score, 1),
        "feedback": "; ".join(feedback) if feedback else "Strong, confident delivery"
    }


def compute_pacing_score(audio_meta: Dict[str, Any], word_count: int) -> Dict[str, Any]:
    """
    Pacing score based on words per minute
    """
    if not audio_meta or not audio_meta.get("duration"):
        return {"score": 7.0, "wpm": None, "feedback": "Duration unavailable"}
    
    duration = audio_meta["duration"]
    wpm = _calculate_wpm(word_count, duration)
    
    if WPM_OPTIMAL_MIN <= wpm <= WPM_OPTIMAL_MAX:
        score = 10
        feedback = f"Perfect pacing ({wpm:.0f} WPM)"
    elif WPM_ACCEPTABLE_MIN <= wpm <= WPM_ACCEPTABLE_MAX:
        score = 8
        feedback = f"Good pacing ({wpm:.0f} WPM)"
    elif wpm < WPM_ACCEPTABLE_MIN:
        score = 6
        feedback = f"Speak a bit faster ({wpm:.0f} WPM - try {WPM_OPTIMAL_MIN}+ WPM)"
    else:
        score = 5
        feedback = f"Slow down slightly ({wpm:.0f} WPM - try under {WPM_OPTIMAL_MAX} WPM)"
    
    return {
        "score": round(score, 1),
        "wpm": round(wpm, 1),
        "feedback": feedback
    }


def sentiment_analysis(text: str) -> Dict[str, Any]:
    """
    Sentiment analysis using transformers or TextBlob fallback
    """
    if not text or len(text.strip()) < 3:
        return {"label": "Neutral", "confidence": 0.5, "mapped_score": 5}
    
    # Try transformers first
    if sentiment_pipeline:
        try:
            result = sentiment_pipeline(text[:512])[0]  # Limit to 512 chars
            label = result["label"]
            confidence = result["score"]
            
            if label == "POSITIVE":
                mapped_score = 8
            elif label == "NEGATIVE":
                mapped_score = 4
            else:
                mapped_score = 6
            
            return {
                "label": label.capitalize(),
                "confidence": round(confidence, 3),
                "mapped_score": mapped_score
            }
        except Exception as e:
            print(f"Sentiment pipeline error: {e}")
    
    # Fallback to TextBlob
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    
    if polarity > 0.1:
        label = "Positive"
        mapped_score = min(8, 6 + polarity * 5)
    elif polarity < -0.1:
        label = "Negative"
        mapped_score = max(3, 5 + polarity * 5)
    else:
        label = "Neutral"
        mapped_score = 6
    
    return {
        "label": label,
        "polarity": round(polarity, 3),
        "mapped_score": round(mapped_score, 1)
    }


def semantic_similarity_score(candidate: str, reference: Optional[str]) -> Optional[float]:
    """
    Semantic similarity using SBERT
    """
    if not reference or not candidate:
        return None
    
    if not SBERT:
        # Simple word overlap fallback
        cand_words = set(re.findall(r"\w+", candidate.lower()))
        ref_words = set(re.findall(r"\w+", reference.lower()))
        intersection = len(cand_words & ref_words)
        union = len(ref_words)
        score = (intersection / max(1, union)) * 10
        return round(min(10, score), 1)
    
    try:
        emb1 = SBERT.encode(candidate, convert_to_tensor=True)
        emb2 = SBERT.encode(reference, convert_to_tensor=True)
        cosine_sim = sbert_util.cos_sim(emb1, emb2).item()
        
        # Map [-1, 1] to [0, 10]
        score = ((cosine_sim + 1) / 2) * 10
        return round(min(10, max(0, score)), 2)
    except Exception as e:
        print(f"Semantic similarity error: {e}")
        return None


# =================== MAIN ANALYSIS FUNCTION ===================

def analyze_response(
    text: str,
    audio_meta: Optional[Dict[str, Any]] = None,
    reference_answer: Optional[str] = None
) -> Dict[str, Any]:
    """
    Main function to analyze interview response and generate comprehensive feedback
    """
    if not text or len(text.strip()) < 3:
        return {
            "error": "Response too short to analyze",
            "overall_performance": 0,
            "overall_feedback": "Please provide a more detailed response."
        }
    
    audio_meta = audio_meta or {}
    word_count = _count_words(text)
    filler_words = _detect_fillers(text)
    filler_count = len(filler_words)
    slang_words = _detect_slang(text)
    
    # Calculate all scores
    clarity_result = compute_clarity_score(text, audio_meta)
    fluency_result = compute_fluency_score(text, filler_count, audio_meta)
    grammar_result = compute_grammar_score(text)
    confidence_result = compute_confidence_score(audio_meta, word_count)
    pacing_result = compute_pacing_score(audio_meta, word_count)
    sentiment_result = sentiment_analysis(text)
    semantic_sim = semantic_similarity_score(text, reference_answer)
    
    # Overall performance (weighted average)
    weights = {
        "clarity": 0.20,
        "fluency": 0.20,
        "grammar": 0.20,
        "confidence": 0.20,
        "pacing": 0.10,
        "sentiment": 0.10
    }
    
    overall = (
        clarity_result["score"] * weights["clarity"] +
        fluency_result["score"] * weights["fluency"] +
        grammar_result["score"] * weights["grammar"] +
        confidence_result["score"] * weights["confidence"] +
        pacing_result["score"] * weights["pacing"] +
        sentiment_result["mapped_score"] * weights["sentiment"]
    )
    
    # Bonus for semantic similarity
    if semantic_sim and semantic_sim > 7:
        overall = min(10, overall + 0.5)
    
    # Generate overall feedback
    feedback_parts = []
    
    if grammar_result["score"] < 6:
        feedback_parts.append(grammar_result.get("feedback", "Review grammar basics"))
    if fluency_result["score"] < 6:
        feedback_parts.append(fluency_result.get("feedback", "Improve fluency"))
    if clarity_result["score"] < 6:
        feedback_parts.append(clarity_result.get("feedback", "Speak more clearly"))
    if confidence_result["score"] >= 8:
        feedback_parts.append("Strong confident delivery!")
    if pacing_result["score"] >= 8:
        feedback_parts.append("Excellent pacing")
    
    if not feedback_parts:
        if overall >= 8:
            feedback_parts.append("Excellent response overall!")
        elif overall >= 6:
            feedback_parts.append("Good response, keep practicing!")
        else:
            feedback_parts.append("Keep practicing to improve")
    
    return {
        "clarity_score": clarity_result["score"],
        "fluency_score": fluency_result["score"],
        "grammar": {
            "score": grammar_result["score"],
            "errors": grammar_result.get("errors", 0),
            "details": grammar_result.get("details", [])
        },
        "confidence_score": confidence_result["score"],
        "pacing_score": pacing_result["score"],
        "pacing_wpm": pacing_result.get("wpm"),
        "sentiment": sentiment_result,
        "semantic_similarity": semantic_sim,
        "filler_word_count": filler_count,
        "filler_words": list(set(filler_words)),
        "slang_detected": list(set(slang_words)),
        "word_count": word_count,
        "duration": audio_meta.get("duration"),
        "avg_pitch": audio_meta.get("avg_pitch"),
        "overall_performance": round(overall, 1),
        "overall_feedback": " ".join(feedback_parts),
        "detailed_feedback": {
            "clarity": clarity_result.get("feedback"),
            "fluency": fluency_result.get("feedback"),
            "grammar": grammar_result.get("feedback"),
            "confidence": confidence_result.get("feedback"),
            "pacing": pacing_result.get("feedback")
        }
    }